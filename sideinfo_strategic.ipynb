{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67eb51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# --- 1. Model Architecture (No changes here) ---\n",
    "class BaseModel:\n",
    "    def __init__(self):\n",
    "        self.w_A = None; self.w_B = None\n",
    "    def _prepare_features(self, X, Z=None):\n",
    "        X_b = np.c_[np.ones(X.shape[0]), X]\n",
    "        if Z is not None: XZ_b = np.c_[X_b, Z]; return XZ_b, X_b\n",
    "        return None, X_b\n",
    "\n",
    "    def predict_A(self, X, Z):\n",
    "        XZ_b, _ = self._prepare_features(X, Z); logits = XZ_b @ self.w_A\n",
    "        prediction = np.sign(logits); prediction[prediction == 0] = 1; return prediction\n",
    "    def predict_B(self, X):\n",
    "        _, X_b = self._prepare_features(X); logits = X_b @ self.w_B\n",
    "        prediction = np.sign(logits); prediction[prediction == 0] = 1; return prediction\n",
    "\n",
    "    def get_strategic_choice_and_prediction(self, X, Z):\n",
    "        XZ_b, X_b = self._prepare_features(X, Z)\n",
    "        logits_A = XZ_b @ self.w_A\n",
    "        logits_B = X_b @ self.w_B\n",
    "        pred_A = np.sign(logits_A); pred_A[pred_A == 0] = 1\n",
    "        pred_B = np.sign(logits_B); pred_B[pred_B == 0] = 1\n",
    "        final_prediction = np.zeros(X.shape[0], dtype=int)\n",
    "        choice_is_A = np.zeros(X.shape[0], dtype=bool)\n",
    "        both_predict_pos = (pred_A == 1) & (pred_B == 1)\n",
    "        final_prediction[both_predict_pos] = 1\n",
    "        choice_is_A[both_predict_pos] = True\n",
    "        only_A_predict_pos = (pred_A == 1) & (pred_B == -1)\n",
    "        final_prediction[only_A_predict_pos] = 1\n",
    "        choice_is_A[only_A_predict_pos] = True\n",
    "        only_B_predict_pos = (pred_A == -1) & (pred_B == 1)\n",
    "        final_prediction[only_B_predict_pos] = 1\n",
    "        choice_is_A[only_B_predict_pos] = False\n",
    "        both_predict_neg = (pred_A == -1) & (pred_B == -1)\n",
    "        final_prediction[both_predict_neg] = -1\n",
    "        choice_is_A[both_predict_neg] = True\n",
    "        return final_prediction, choice_is_A\n",
    "\n",
    "    def predict_strategic_with_choice(self, X, Z):\n",
    "        prediction, choice_is_A = self.get_strategic_choice_and_prediction(X, Z)\n",
    "        color_grid = np.zeros_like(prediction, dtype=int)\n",
    "        color_grid[(prediction == 1) & choice_is_A] = 2\n",
    "        color_grid[(prediction == 1) & ~choice_is_A] = 1\n",
    "        color_grid[(prediction == -1) & ~choice_is_A] = -1\n",
    "        color_grid[(prediction == -1) & choice_is_A] = -2\n",
    "        return color_grid\n",
    "\n",
    "    def evaluate(self, X, Y, Z, strategic=False):\n",
    "        if not strategic:\n",
    "            XZ_b, _ = self._prepare_features(X, Z)\n",
    "            final_logits = XZ_b @ self.w_A\n",
    "        else:\n",
    "            final_logits, _ = self.get_strategic_choice_and_prediction(X, Z)\n",
    "        y_pred = np.sign(final_logits)\n",
    "        y_pred[y_pred == 0] = 1\n",
    "        return accuracy_score(Y, y_pred)\n",
    "\n",
    "    def _logistic_loss(self, y_true, logits): return np.mean(np.log(1 + np.exp(-y_true * logits)))\n",
    "\n",
    "class StrategicModel(BaseModel):\n",
    "    def __init__(self, optimizer_method='Nelder-Mead', optimizer_maxiter=500, optimizer_disp=False):\n",
    "        super().__init__(); self.optimizer_method = optimizer_method; self.optimizer_maxiter = optimizer_maxiter; self.optimizer_disp = optimizer_disp\n",
    "    def _objective(self, weights, X, Y, Z):\n",
    "        self.w_A = weights[:4]; self.w_B = weights[4:]; XZ_b, X_b = self._prepare_features(X, Z)\n",
    "        logits_A = XZ_b @ self.w_A; logits_B = X_b @ self.w_B\n",
    "        use_A = logits_A >= logits_B\n",
    "        final_logits = np.where(use_A, logits_A, logits_B); return self._logistic_loss(Y, final_logits)\n",
    "    def train(self, X, Y, Z):\n",
    "        initial_weights = np.zeros(7)\n",
    "        result = minimize(self._objective, initial_weights, args=(X, Y, Z), method=self.optimizer_method, options={'maxiter': self.optimizer_maxiter, 'disp': self.optimizer_disp})\n",
    "        self.w_A = result.x[:4]; self.w_B = result.x[4:]\n",
    "\n",
    "class VanillaModel(BaseModel):\n",
    "    def __init__(self, optimizer_method='Nelder-Mead', optimizer_maxiter=500, optimizer_disp=False):\n",
    "        super().__init__(); self.optimizer_method = optimizer_method; self.optimizer_maxiter = optimizer_maxiter; self.optimizer_disp = optimizer_disp\n",
    "    def _objective_A(self, w_A, XZ_b, Y): return self._logistic_loss(Y, XZ_b @ w_A)\n",
    "    def _objective_B(self, w_B, X_b, Y): return self._logistic_loss(Y, X_b @ w_B)\n",
    "    def train(self, X, Y, Z):\n",
    "        XZ_b, X_b = self._prepare_features(X, Z)\n",
    "        res_A = minimize(self._objective_A, np.zeros(4), args=(XZ_b, Y), method=self.optimizer_method, options={'maxiter': self.optimizer_maxiter, 'disp': self.optimizer_disp})\n",
    "        self.w_A = res_A.x\n",
    "        res_B = minimize(self._objective_B, np.zeros(3), args=(X_b, Y), method=self.optimizer_method, options={'maxiter': self.optimizer_maxiter, 'disp': self.optimizer_disp})\n",
    "        self.w_B = res_B.x\n",
    "\n",
    "class BayesianModel(BaseModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.yz_pairs = None; self.yz_probabilities = None\n",
    "        self.centers = None; self.covariances = None\n",
    "\n",
    "    def learn_parameters_from_dataset(self, dataset: 'Dataset'):\n",
    "        self.yz_pairs = dataset.yz_pairs\n",
    "        self.yz_probabilities = dataset.yz_probabilities\n",
    "        self.centers = dataset.centers\n",
    "        self.covariances = dataset.covariances\n",
    "        if hasattr(self, 'verbose') and self.verbose:\n",
    "            print(\"\\n--- Bayesian Model initialized with true generative parameters (Calculated, not learned) ---\")\n",
    "\n",
    "    def _get_joint_prob(self, y, z):\n",
    "        for i, pair in enumerate(self.yz_pairs):\n",
    "            if pair[0] == y and pair[1] == z:\n",
    "                return self.yz_probabilities[i]\n",
    "        return 0\n",
    "\n",
    "    def predict_proba(self, X, Z):\n",
    "        n_samples = X.shape[0]; prob_Y_is_1 = np.zeros(n_samples)\n",
    "        for i in range(n_samples):\n",
    "            x_i, z_i = X[i, :], Z[i]\n",
    "            mean_y1_z, cov_y1_z = self.centers.get((1, z_i)), self.covariances.get((1, z_i))\n",
    "            mean_y_neg1_z, cov_y_neg1_z = self.centers.get((-1, z_i)), self.covariances.get((-1, z_i))\n",
    "            if mean_y1_z is None or mean_y_neg1_z is None:\n",
    "                prob_Y_is_1[i] = 0.5; continue\n",
    "            likelihood_y1 = multivariate_normal.pdf(x_i, mean=mean_y1_z, cov=cov_y1_z)\n",
    "            likelihood_y_neg1 = multivariate_normal.pdf(x_i, mean=mean_y_neg1_z, cov=cov_y_neg1_z)\n",
    "            joint_prob_y1_z = self._get_joint_prob(1, z_i)\n",
    "            joint_prob_y_neg1_z = self._get_joint_prob(-1, z_i)\n",
    "            numerator_y1 = likelihood_y1 * joint_prob_y1_z\n",
    "            numerator_y_neg1 = likelihood_y_neg1 * joint_prob_y_neg1_z\n",
    "            denominator = numerator_y1 + numerator_y_neg1\n",
    "            prob_Y_is_1[i] = 0.5 if denominator < 1e-9 else numerator_y1 / denominator\n",
    "        return prob_Y_is_1\n",
    "\n",
    "    def predict(self, X, Z):\n",
    "        prob_Y_is_1 = self.predict_proba(X, Z)\n",
    "        return np.where(prob_Y_is_1 >= 0.5, 1, -1)\n",
    "\n",
    "    def evaluate(self, X, Y, Z):\n",
    "        y_pred = self.predict(X, Z)\n",
    "        return accuracy_score(Y, y_pred)\n",
    "\n",
    "# --- 2. Data Handling (No changes here) ---\n",
    "class Dataset:\n",
    "    def __init__(self, n_train, n_test, rng, parameter_mode='random', verbose=True):\n",
    "        self.n_train = n_train; self.n_test = n_test; self.rng = rng\n",
    "        self.parameter_mode = parameter_mode\n",
    "        self.verbose = verbose\n",
    "        self._generate_data_distribution_parameters()\n",
    "        self.X_train, self.Y_train, self.Z_train = None, None, None\n",
    "        self.X_test, self.Y_test, self.Z_test = None, None, None\n",
    "\n",
    "    def _generate_data_distribution_parameters(self):\n",
    "        self.yz_pairs = np.array([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n",
    "\n",
    "        if self.parameter_mode == 'manual':\n",
    "            if self.verbose: print(\"\\n--- Using MANUALLY DEFINED Data Distribution Parameters ---\")\n",
    "            self.yz_probabilities = np.array([0.23, 0.27, 0.22, 0.28])\n",
    "            self.centers = {\n",
    "                (-1, -1): np.array([-0.6, -0.6]), (-1, 1):  np.array([-0.65, -0.55]),\n",
    "                (1, -1):  np.array([-0.5, -0.6]),  (1, 1):   np.array([-0.6, 0.55])\n",
    "            }\n",
    "            self.covariances = {\n",
    "                (-1, -1): np.array([[0.1, 0.04], [0.04, 0.08]]), (-1, 1):  np.array([[0.08, -0.04], [-0.04, 0.1]]),\n",
    "                (1, -1):  np.array([[0.08, 0.04], [0.04, 0.1]]),  (1, 1):   np.array([[0.1, -0.04], [-0.04, 0.08]])\n",
    "            }\n",
    "        elif self.parameter_mode == 'random':\n",
    "            if self.verbose: print(\"\\n--- Using RANDOMLY GENERATED Data Distribution Parameters ---\")\n",
    "            self.yz_probabilities = self.rng.dirichlet(np.ones(4))\n",
    "            self.centers = {tuple(p): self.rng.uniform(-0.8, 0.8, size=2) for p in self.yz_pairs}\n",
    "            self.covariances = {}\n",
    "            for pair in self.yz_pairs:\n",
    "                A = self.rng.uniform(-0.5, 0.5, size=(2, 2)); cov = A @ A.T + np.eye(2) * 0.01\n",
    "                self.covariances[tuple(pair)] = cov * self.rng.uniform(0.05, 0.15)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid parameter_mode: {self.parameter_mode}. Must be 'manual' or 'random'.\")\n",
    "\n",
    "    def _sample_from_distribution(self, n_samples):\n",
    "        yz_idx = self.rng.choice(4, size=n_samples, p=self.yz_probabilities); Y = self.yz_pairs[yz_idx, 0]; Z = self.yz_pairs[yz_idx, 1]\n",
    "        X = np.zeros((n_samples, 2))\n",
    "        for i in range(n_samples):\n",
    "            mean = self.centers[(Y[i], Z[i])]; cov = self.covariances[(Y[i], Z[i])]\n",
    "            X[i, :] = self.rng.multivariate_normal(mean, cov=cov)\n",
    "        return X, Y, Z\n",
    "    def generate_data(self):\n",
    "        self.X_train, self.Y_train, self.Z_train = self._sample_from_distribution(self.n_train)\n",
    "        self.X_test, self.Y_test, self.Z_test = self._sample_from_distribution(self.n_test)\n",
    "        if self.verbose: print(f\"Generated {self.n_train} training samples and {self.n_test} test samples.\")\n",
    "\n",
    "# --- 3. Experiment Orchestration (MODIFIED) ---\n",
    "class Experiment:\n",
    "    def __init__(self, n_train, n_test, seed, optimizer_method='Nelder-Mead', optimizer_maxiter=500, optimizer_disp=False, parameter_mode='random', verbose=True):\n",
    "        self.n_train = n_train; self.n_test = n_test; self.seed = seed\n",
    "        self.verbose = verbose\n",
    "        self.rng = np.random.default_rng(self.seed)\n",
    "        self.optimizer_method = optimizer_method; self.optimizer_maxiter = optimizer_maxiter; self.optimizer_disp = optimizer_disp\n",
    "        self.dataset = Dataset(self.n_train, self.n_test, self.rng, parameter_mode, verbose=self.verbose)\n",
    "        self.strategic_model = StrategicModel(optimizer_method, optimizer_maxiter, optimizer_disp)\n",
    "        self.vanilla_model = VanillaModel(optimizer_method, optimizer_maxiter, optimizer_disp)\n",
    "        self.bayesian_model = BayesianModel()\n",
    "        self.bayesian_model.verbose = self.verbose # Pass verbose flag down\n",
    "\n",
    "    def _print_data_parameters(self):\n",
    "        print(\"\\n--- Shared Data Generation Parameters (Seed: {}) ---\".format(self.seed))\n",
    "        probs_str = np.array2string(self.dataset.yz_probabilities, precision=4, suppress_small=True)\n",
    "        print(\"P(Y,Z) for [(-1,-1), (-1,1), (1,-1), (1,1)]: {}\".format(probs_str))\n",
    "        print(\"Conditional Gaussian P(X | Y, Z):\")\n",
    "        for pair in self.dataset.yz_pairs:\n",
    "            pair_tuple = tuple(pair)\n",
    "            center, cov = self.dataset.centers[pair_tuple], self.dataset.covariances[pair_tuple]\n",
    "            mean_str = np.array2string(center, precision=3, suppress_small=True, separator=',')\n",
    "            cov_str = np.array2string(cov, prefix=' '*10, precision=3, suppress_small=True, separator=',').replace('\\n', ' ').replace('  ', ' ')\n",
    "            print(f\"  (Y={pair[0]:>2}, Z={pair[1]:>2}) -> Mean: {mean_str}, Covariance: {cov_str}\")\n",
    "\n",
    "    # MODIFIED: Added a `verbose` flag and a return value\n",
    "    def run_training_and_evaluation(self):\n",
    "        if self.verbose: print(f\"--- Generating Data for seed number {self.seed}---\")\n",
    "        self.dataset.generate_data()\n",
    "        if self.verbose: self._print_data_parameters()\n",
    "\n",
    "        if self.verbose: print(\"\\n--- Training Models ---\")\n",
    "        self.strategic_model.train(self.dataset.X_train, self.dataset.Y_train, self.dataset.Z_train)\n",
    "        self.vanilla_model.train(self.dataset.X_train, self.dataset.Y_train, self.dataset.Z_train)\n",
    "        self.bayesian_model.learn_parameters_from_dataset(self.dataset)\n",
    "        if self.verbose: print(\"Training complete.\")\n",
    "\n",
    "        if self.verbose: print(\"\\n--- Evaluating Models ---\")\n",
    "        results = []\n",
    "        for name, model in [('Strategic', self.strategic_model), ('Vanilla', self.vanilla_model)]:\n",
    "            acc_ns = model.evaluate(self.dataset.X_test, self.dataset.Y_test, self.dataset.Z_test, strategic=False)\n",
    "            acc_s = model.evaluate(self.dataset.X_test, self.dataset.Y_test, self.dataset.Z_test, strategic=True)\n",
    "            results.append({'Model': name, 'Non-Strategic Acc': acc_ns, 'Strategic Acc': acc_s})\n",
    "\n",
    "        acc_bayesian = self.bayesian_model.evaluate(self.dataset.X_test, self.dataset.Y_test, self.dataset.Z_test)\n",
    "        results.append({'Model': 'Bayesian (Optimal)', 'Non-Strategic Acc': acc_bayesian, 'Strategic Acc': np.nan})\n",
    "\n",
    "        results_df = pd.DataFrame(results).round(4)\n",
    "        if self.verbose:\n",
    "            print(\"\\n--- Evaluation Results ---\"); print(results_df.to_string(index=False))\n",
    "\n",
    "        return results_df\n",
    "\n",
    "# --- 4. New Functions for Multiple Runs and Plotting ---\n",
    "\n",
    "def run_multiple_experiments(seeds, params):\n",
    "    \"\"\"\n",
    "    Runs the experiment for a list of seeds and collects the key accuracy metrics.\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"--- Running experiment for seed: {seed} ---\")\n",
    "        # Create a new experiment instance for each seed\n",
    "        experiment = Experiment(\n",
    "            n_train=params['n_train'],\n",
    "            n_test=params['n_test'],\n",
    "            seed=seed,\n",
    "            optimizer_method=params['optimizer_method'],\n",
    "            optimizer_maxiter=params['optimizer_maxiter'],\n",
    "            optimizer_disp=params['optimizer_disp'],\n",
    "            parameter_mode=params['parameter_mode'],\n",
    "            verbose=False # Suppress detailed output for each run\n",
    "        )\n",
    "\n",
    "        # Run training and evaluation to get the results DataFrame for this seed\n",
    "        results_df = experiment.run_training_and_evaluation()\n",
    "\n",
    "        # Extract the 4 required values\n",
    "        strat_row = results_df[results_df['Model'] == 'Strategic']\n",
    "        vanilla_row = results_df[results_df['Model'] == 'Vanilla']\n",
    "\n",
    "        result_entry = {\n",
    "            'seed': seed,\n",
    "            'Strategic_Non-Strategic_Acc': strat_row['Non-Strategic Acc'].iloc[0],\n",
    "            'Strategic_Strategic_Acc': strat_row['Strategic Acc'].iloc[0],\n",
    "            'Vanilla_Non-Strategic_Acc': vanilla_row['Non-Strategic Acc'].iloc[0],\n",
    "            'Vanilla_Strategic_Acc': vanilla_row['Strategic Acc'].iloc[0],\n",
    "        }\n",
    "        all_results.append(result_entry)\n",
    "\n",
    "    # Convert the list of dictionaries to a final DataFrame\n",
    "    final_df = pd.DataFrame(all_results)\n",
    "    return final_df\n",
    "\n",
    "def plot_results(df):\n",
    "    \"\"\"\n",
    "    Generates a scatter plot of the results from the multiple experiments.\n",
    "    \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "    # Plot each of the four metrics\n",
    "    ax.scatter(df['seed'], df['Strategic_Non-Strategic_Acc'], label='Strategic Model (Non-Strategic Eval)', marker='o', s=80)\n",
    "    ax.scatter(df['seed'], df['Strategic_Strategic_Acc'], label='Strategic Model (Strategic Eval)', marker='^', s=80)\n",
    "    ax.scatter(df['seed'], df['Vanilla_Non-Strategic_Acc'], label='Vanilla Model (Non-Strategic Eval)', marker='s', s=80)\n",
    "    ax.scatter(df['seed'], df['Vanilla_Strategic_Acc'], label='Vanilla Model (Strategic Eval)', marker='x', s=80, c='red')\n",
    "\n",
    "    ax.set_title('Model Accuracy Across Different Random Seeds', fontsize=16)\n",
    "    ax.set_xlabel('Random Seed', fontsize=12)\n",
    "    ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "    ax.set_xticks(df['seed'])\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.set_ylim(bottom=df.iloc[:, 1:].min().min() - 0.05, top=1.0) # Adjust y-axis for clarity\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- Control Panel ---\n",
    "    PARAMETER_MODE = 'random' # 'manual' or 'random'\n",
    "    N_TRAIN = 5000\n",
    "    N_TEST = 10000\n",
    "    OPTIMIZER_METHOD = 'Nelder-Mead'\n",
    "    OPTIMIZER_MAXITER = 2000\n",
    "    OPTIMIZER_DISP = False\n",
    "\n",
    "    # List of seeds to iterate over\n",
    "    SEEDS_TO_RUN = list(range(1, 21))\n",
    "\n",
    "    # Package parameters into a dictionary\n",
    "    exp_params = {\n",
    "        'n_train': N_TRAIN,\n",
    "        'n_test': N_TEST,\n",
    "        'optimizer_method': OPTIMIZER_METHOD,\n",
    "        'optimizer_maxiter': OPTIMIZER_MAXITER,\n",
    "        'optimizer_disp': OPTIMIZER_DISP,\n",
    "        'parameter_mode': PARAMETER_MODE\n",
    "    }\n",
    "\n",
    "    # --- Run the batch experiment and get the numerical results ---\n",
    "    results_table = run_multiple_experiments(SEEDS_TO_RUN, exp_params)\n",
    "\n",
    "    # --- Display the collected numerical data ---\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\" \" * 20 + \"AGGREGATED RESULTS ACROSS ALL SEEDS\")\n",
    "    print(\"=\"*80)\n",
    "    # Set pandas display options to show all columns and round the output\n",
    "    pd.set_option('display.width', 120)\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "    print(results_table)\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # --- (Optional) Visualize the results as a scatter plot ---\n",
    "    print(\"\\nGenerating scatter plot of the results...\")\n",
    "    plot_results(results_table)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
